{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULdrhOaVbsdO"
      },
      "source": [
        "# Tutorial\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/mibrahimi/rlba/blob/main/examples/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This colab provides an overview of how to use the interface defiend in this library and experiment with existing example agents and environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaJxoatMhJ71"
      },
      "source": [
        "## Installation\n",
        "\n",
        "We'll start by installing the package and required depencencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install necessary dependencies.\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install git+https://github.com/mibrahimi/rlba.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-H2d6UZi7Sf"
      },
      "source": [
        "## Import Modules\n",
        "\n",
        "Now we can import all the relevant modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ74Id-8MERq"
      },
      "outputs": [],
      "source": [
        "#@title Import modules.\n",
        "#python3\n",
        "from dataclasses import dataclass\n",
        "from random import random\n",
        "from typing import Sequence\n",
        "\n",
        "from rlba.environment_loop import EnvironmentLoop\n",
        "from rlba.environments import BernoulliBanditEnv\n",
        "from rlba.types import Array, ArraySpec, BoundedArraySpec, DiscreteArraySpec, NestedArray\n",
        "from rlba.utils import metrics\n",
        "import numpy as np\n",
        "from numpy.random import default_rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FbmoOpZKwid"
      },
      "source": [
        "## Create an agent\n",
        "\n",
        "An `Agent` is the *mind* that interacts and learns from an environment.\n",
        "\n",
        "<img src=\"https://github.com/mibrahimi/rlba/raw/main/docs/imgs/RLProblem.png\" width=\"500\" />\n",
        "\n",
        "You can write your own agent by writing a class that implements the [Agent](https://github.com/mibrahimi/rlba/raw/main/rlba/Agent.py) protocol, i.e., implements the following four methods: `action_spec`, `observation_spec`, `observe`, and `select_action`. As an example, below we implement a greedy agent for Bernoulli bandit environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title A greedy agent for Bernoulli bandit environment.\n",
        "\n",
        "class GreedyBernoulliAgent:\n",
        "  \"\"\"A greedy agent that select action that maximized current point estimate.\"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    n_action: int,\n",
        "    seed: int,\n",
        "    alpha_0: int = 1,\n",
        "    beta_0: int = 1,\n",
        "  ):\n",
        "    self._action_spec: DiscreteArraySpec = DiscreteArraySpec(n_action, name='action spec')\n",
        "    self._observation_spec: ArraySpec = BoundedArraySpec(\n",
        "        shape=(), dtype=np.float32, minimum=0.0, maximum=1.0,\n",
        "        name='observation spec')\n",
        "    self._n_success = np.zeros(shape=(n_action,), dtype=int) + alpha_0\n",
        "    self._n_failure = np.zeros(shape=(n_action,), dtype=int) + beta_0\n",
        "    self._rng = default_rng(seed)\n",
        "    \n",
        "  def select_action(self) -> int:\n",
        "    \"\"\"Samples from the policy and returns an action.\"\"\"\n",
        "    pvals_hat = self._n_success / (self._n_success + self._n_failure)\n",
        "    action = random_argmax(pvals_hat, self._rng)\n",
        "    return action\n",
        "\n",
        "  def reward_spec(self) -> Array:\n",
        "    \"\"\"Describes the reward returned by the environment.\n",
        "    Returns:\n",
        "      An `Array` spec.\n",
        "    \"\"\"\n",
        "    return Array(shape=(), dtype=float, name='reward')\n",
        "\n",
        "  def discount_spec(self) -> BoundedArraySpec:\n",
        "    \"\"\"Describes the discount considered by the agent for planning.\n",
        "\n",
        "    By default this is assumed to be a single float between 0 and 1.\n",
        "\n",
        "    Returns:\n",
        "      An `Array` spec.\n",
        "    \"\"\"\n",
        "    return BoundedArraySpec(\n",
        "        shape=(), dtype=float, minimum=0., maximum=1., name='discount')\n",
        "\n",
        "  def observe(\n",
        "      self,\n",
        "      action: int,\n",
        "      obs: float,\n",
        "  ):\n",
        "    \"\"\"Make an observation from the environment.\n",
        "\n",
        "    Args:\n",
        "      action: action taken in the environment.\n",
        "      obs: observation produced by the environment given the action.\n",
        "    \"\"\"\n",
        "    self._n_success[action] += obs\n",
        "    self._n_failure[action] += (1 - obs)\n",
        "    return obs\n",
        "\n",
        "\n",
        "def random_argmax(vals: Sequence[float], rng):\n",
        "  maxval = max(vals)\n",
        "  argmax = [idx for idx, val in enumerate(vals) if val == maxval]\n",
        "  return rng.choice(argmax)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL6U_Wi2HTA2"
      },
      "source": [
        "## Training loop\n",
        "Finally, we can have the agent interact with the environment in an environment loop and evaluate its performance. We use the Bernoulli bandit implementation [here](https://github.com/mibrahimi/rlba/raw/main/rlba/environments/bernoulli_bandit.py) as the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pvals = [0, 0.55, 0.5]\n",
        "n_action = len(pvals)\n",
        "seed=0\n",
        "env = BernoulliBanditEnv(pvals, seed)\n",
        "agent = GreedyBernoulliAgent(n_action, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loop = EnvironmentLoop(env, agent)\n",
        "loop.run(100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VEEj3Qw60y73"
      ],
      "name": "RLBA: Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 64-bit ('rlba')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "18c1406e75f503466fbede59d03eb242d648e983d4ce59fc7cfb3cf3b716f936"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
